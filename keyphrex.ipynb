{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/ heuristic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We limit ourselves to only noun phrases matching the POS pattern ```{(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}``` (a regular expression written in a simplified format used by NLTKâ€™s RegexpParser()). This matches any number of adjectives followed at least by one noun that may be joined by a preposition to (optionally) any number of adjectives followed by other noun(s) sequence, and results in the following candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    with open(file) as text:\n",
    "        doc = text.read()\n",
    "    return doc\n",
    "\n",
    "def extract_candidate_chunks(text_string, max_words=3):\n",
    "    # Any number of adjectives followed by noun(s) and (optionally) joined\n",
    "    # by a preposition to any number of adjectives followed by any number of nouns\n",
    "    grammar = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "    \n",
    "    # Exclude candidates that are stop words or punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # Make chunk using regular expression\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    # Tokenize and POS-tag\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text_string))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    \n",
    "    # Join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda l: l[2] != 'O') if key]\n",
    "\n",
    "     # Filter by maximum keyphrase length\n",
    "    candidates = list(filter(lambda l: len(l.split()) <= 3, candidates))\n",
    "    \n",
    "    candidates = [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def score_keyphrases_tfidf(text_files, number_of_terms=10, max_words=3):    \n",
    "    # Extract candidate chunks from each text in text_files\n",
    "    chunked_texts = [extract_candidate_chunks(read_txt(text), max_words=max_words) for text in text_files]\n",
    "\n",
    "    # Map id and term\n",
    "    dictionary = gensim.corpora.Dictionary(chunked_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in chunked_texts]\n",
    "    \n",
    "    # tf*idf frequency model\n",
    "    tfidf = gensim.models.TfidfModel(corpus[0:], normalize=False, wglobal=lambda df, D: math.log((1 + D) / (1 + df)) + 1)\n",
    "    corpus_tfidf = tfidf[corpus][0]\n",
    "    \n",
    "    # Sort by score \n",
    "    sorted_corpus = sorted(corpus_tfidf, key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Compute top n terms\n",
    "    top_terms = [(dictionary[s[0]], s[1]) for s in sorted_corpus]\n",
    "\n",
    "    return top_terms[:number_of_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we assign texts to the list of transcripts, then we call score_keyphrases_by_tfidf(texts) to get all transcripts back in a sparse, tf`*`idf-weighted representation. Then we print out the 10 candidate keyphrases with the highest tf`*`idf values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 46.0),\n",
       " ('foods', 15.0),\n",
       " ('people', 13.454579064456308),\n",
       " ('countries', 12.086604990127926),\n",
       " ('example', 11.008291961827886),\n",
       " ('prices', 11.008291961827886),\n",
       " ('animals', 10.575779366361935),\n",
       " ('many cultures', 9.581453659370776),\n",
       " ('farmers', 9.581453659370776),\n",
       " ('flavor', 9.064953742595945)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = 'scripts'\n",
    "fns = ['script.txt', 'transcript_1.txt', 'transcript_2.txt', 'transcript_3.txt']\n",
    "text_files = [os.path.join(sp, fn) for fn in fns]\n",
    "score_keyphrases_tfidf(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrase....score:\n",
      "food: 46.0\n",
      "\n",
      "foods: 15.0\n",
      "\n",
      "people: 13.5\n",
      "\n",
      "countries: 12.1\n",
      "\n",
      "example: 11.0\n",
      "\n",
      "prices: 11.0\n",
      "\n",
      "animals: 10.6\n",
      "\n",
      "many cultures: 9.6\n",
      "\n",
      "farmers: 9.6\n",
      "\n",
      "flavor: 9.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyphrases = score_keyphrases_tfidf(text_files)\n",
    "# Print top keywords by TF-IDF\n",
    "print(\"Keyphrase....score:\")\n",
    "\n",
    "for term, score in keyphrases:\n",
    "    print(\"{}: {:0.1f}\".format(term, score))\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
